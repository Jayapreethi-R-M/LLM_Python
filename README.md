# LLM_Python

Hello! Thank you for visiting my repository.

This project contains the code for a Python-based QA (Question-Answer) system that I developed to assist students in their research and academic writing process.

‚ú® Project Overview
  In academic environments, students and researchers frequently perform repetitive tasks that can disrupt their writing process. These tasks range from checking sentence structure to verifying proper citation formats. To address this, I built an LLM-powered application using PyCharm and integrated the Groq API, specifically the LLaMA3-70b-8192 model, to leverage its AI capabilities.

  This model helps users quickly generate relevant and concise academic writing support ‚Äî including grammar correction, topic idea generation, short research answers, and citation formatting ‚Äî streamlining common writing challenges. To create a user-friendly experience, I used Streamlit to design an intuitive and interactive front end. The system was evaluated using custom performance metrics making sure that the outputs were both useful and reliable for academic purposes.

‚ú® Functions
  The LLM has four key functionalities:
    * Grammar Check - Students can paste their text into the platform, and the AI will provide grammar corrections and writing improvement suggestions.
    * Topic Generator - By entering an area of study, users get five specific research topics and ideas to guide their writing process.
    * Research Helper - Students can clarify research-related concepts and get concise, well-structured paragraph responses to aid their understanding.
    * Citation Generator - Users can input relevant information about academic papers, and the AI will generate citations in both APA and MLA formats.

Here's the initial view of the Streamlit webpage: ![Screenshot 2025-06-20 at 10 41 07 PM](https://github.com/user-attachments/assets/9cd43762-e551-4c43-957f-35a311395bd6)


üîç More on the Evaluation....
  To evaluate the model performance, I focused on the two most open-ended functions: Answering Research Questions and Generating Topic Ideas. For the more deterministic tasks like ‚Äî Grammar Checking and Citation Generation ‚Äî I conducted a manual inspection to verify correctness, and they performed reliably in most cases.
  1. Research Question Answering
      To assess the quality of generated answers, I used two key metrics:
        * ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation):‚Ä®This evaluates the overlap between the generated answer and a reference (ideal) answer based on longest common sequences. I chose ROUGE-L because it captures both precision (how much of the generated answer matches the reference) and recall (how much of the reference is covered).
        * Semantic Similarity (using Sentence Transformers):‚Ä®This measures how semantically close the generated answer is to a reference, even if the wording differs. It reflects deeper understanding beyond surface-level overlap.
    I ran the same prompts through state-of-the-art LLMs like ChatGPT and Gemini, then compared their outputs using these metrics. Interestingly, the responses generated by our model showed higher semantic similarity with ChatGPT, suggesting alignment in how both models approach explanation. The ROUGE scores varied at times but generally indicated similar performance.

  2. Topic Idea Generation
      To evaluate how well the generated topics aligned with the user's main concept, I used:
        * Context Similarity (Cosine Similarity):‚Ä®This compared embeddings of the user's topic in the input prompt with the generated research ideas using Sentence Transformers. The goal was to quantify how contextually relevant the ideas were.
    In testing, I found that context similarity was higher when the input topic was clearly phrased and focused on key terms. However, scores dropped significantly when the input combined two unrelated ideas (e.g., "Climate Change and Ancient Literature") or was overly vague.‚Ä®This suggests the model performs best when the prompt is focused, likely because LLMs generate ideas by anchoring strongly to semantic proximity ‚Äî and when topics drift, their ability to maintain relevance gets worse.

  Overall, the model is capable of generating high-quality academic responses and ideas, especially when provided with clearly defined inputs. The evaluation metrics support its effectiveness, particularly in tasks requiring semantic understanding. While it's not flawless, its output is often comparable to leading models, making it a helpful tool for students and researchers.

üîß Future Improvements
    Here are several improvements that can enhance both functionality and user experience:
      * Enable the model to remember previous prompts and responses or remember key information suggested by the user.
      * Allow users to choose from more citation styles like Chicago, Harvard, or IEEE.
      * Extend the topic generation feature to include a structured outline for academic writing.
      * Improve upon the UI, to make it much more streamlined and user friendly.
      * Use much complex evaluation metrics (Including RAG pipeline) to fine tune the model.

üì¨ Contact
If you have any questions or feedback, please feel free to open an issue or reach out. Thank you!
